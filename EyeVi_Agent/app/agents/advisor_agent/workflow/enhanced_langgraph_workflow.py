#!/usr/bin/env python3
"""
Enhanced LangGraph Workflow cho Eyewear Advisor Agent
- Advanced intent detection vÃ  routing
- Multi-step reasoning vá»›i memory
- Dynamic parameter adjustment
- Sophisticated error handling vÃ  recovery
- State management vá»›i history
"""

from typing import Dict, List, Any, TypedDict, Literal, Optional
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from pydantic import BaseModel, Field
import time
import json

from utils.embedding_manager import EmbeddingManager
from utils.qdrant_manager import QdrantManager
from agents.rag_agent import RAGAgent
from config import Config

# Intent Detection Models
class QueryIntent(BaseModel):
    """Structured output for intent detection"""
    intent_type: Literal["medical_consultation", "product_recommendation", "technical_advice", "style_consultation", "general_inquiry", "complex_analysis"] = Field(
        description="Type of user query intent"
    )
    confidence: float = Field(description="Confidence score 0-1", ge=0, le=1)
    sub_intents: List[str] = Field(description="Secondary intent categories", default=[])
    complexity_level: Literal["simple", "moderate", "complex", "expert"] = Field(description="Query complexity")
    requires_multi_step: bool = Field(description="Whether query needs multi-step reasoning")
    key_entities: List[str] = Field(description="Important entities extracted from query", default=[])

class WorkflowState(TypedDict):
    """Enhanced state cho LangGraph workflow"""
    # Core input
    query: str
    user_context: Dict[str, Any]  # User profile, preferences, history
    
    # Intent analysis
    intent: Optional[QueryIntent]
    processing_strategy: str
    
    # Multi-step reasoning
    reasoning_steps: List[Dict[str, Any]]
    current_step: int
    sub_queries: List[str]
    
    # Retrieval & processing
    query_embedding: List[float]
    retrieved_documents: List[Dict]
    filtered_documents: List[Dict]
    context_documents: List[Dict]
    
    # Response generation
    intermediate_answers: List[str]
    final_answer: str
    sources: List[str]
    confidence_score: float
    
    # State management
    messages: List[BaseMessage]
    step_history: List[str]
    error_count: int
    retry_count: int
    
    # Metadata
    processing_time: float
    tokens_used: int
    status: str

class EnhancedEyewearWorkflow:
    """
    Enhanced LangGraph workflow vá»›i advanced features:
    - Intelligent intent detection
    - Dynamic routing based on intent
    - Multi-step reasoning capabilities
    - Sophisticated error handling
    - State memory management
    """
    
    def __init__(self):
        print("ðŸš€ Initializing Enhanced LangGraph Workflow...")
        
        # Initialize components
        self.embedding_manager = EmbeddingManager()
        self.qdrant_manager = QdrantManager()
        self.rag_agent = RAGAgent()
        
        # Initialize LLM for intent detection
        self.intent_llm = ChatGoogleGenerativeAI(
            model=Config.GEMINI_MODEL,
            temperature=0.1,  # Low temperature for consistent intent detection
            google_api_key=Config.GOOGLE_API_KEY
        )
        
        # Initialize reasoning LLM
        self.reasoning_llm = ChatGoogleGenerativeAI(
            model=Config.GEMINI_MODEL,
            temperature=0.3,
            google_api_key=Config.GOOGLE_API_KEY
        )
        
        # Create workflow
        self.workflow = self._create_enhanced_workflow()
        self.compiled_workflow = self.workflow.compile()
        
        print("âœ… Enhanced LangGraph Workflow ready!")
    
    def _create_enhanced_workflow(self) -> StateGraph:
        """Create enhanced workflow graph with sophisticated routing"""
        workflow = StateGraph(WorkflowState)
        
        # Core processing nodes
        workflow.add_node("analyze_intent", self.analyze_intent_node)
        workflow.add_node("plan_strategy", self.plan_strategy_node)
        workflow.add_node("retrieve_context", self.retrieve_context_node)
        workflow.add_node("filter_documents", self.filter_documents_node)
        
        # Specialized processing nodes
        workflow.add_node("medical_consultation", self.medical_consultation_node)
        workflow.add_node("product_recommendation", self.product_recommendation_node)
        workflow.add_node("technical_analysis", self.technical_analysis_node)
        workflow.add_node("style_consultation", self.style_consultation_node)
        workflow.add_node("complex_reasoning", self.complex_reasoning_node)
        workflow.add_node("general_response", self.general_response_node)
        
        # Multi-step processing
        workflow.add_node("decompose_query", self.decompose_query_node)
        workflow.add_node("process_sub_query", self.process_sub_query_node)
        workflow.add_node("synthesize_answers", self.synthesize_answers_node)
        
        # Finalization
        workflow.add_node("finalize_response", self.finalize_response_node)
        workflow.add_node("handle_error", self.handle_error_node)
        
        # Set entry point
        workflow.set_entry_point("analyze_intent")
        
        # Intent analysis flow
        workflow.add_edge("analyze_intent", "plan_strategy")
        
        # Strategy planning with conditional routing
        workflow.add_conditional_edges(
            "plan_strategy",
            self.route_by_strategy,
            {
                "simple_retrieval": "retrieve_context",
                "multi_step": "decompose_query",
                "error": "handle_error"
            }
        )
        
        # Document processing flow
        workflow.add_edge("retrieve_context", "filter_documents")
        
        # Intent-based routing
        workflow.add_conditional_edges(
            "filter_documents", 
            self.route_by_intent,
            {
                "medical_consultation": "medical_consultation",
                "product_recommendation": "product_recommendation", 
                "technical_advice": "technical_analysis",
                "style_consultation": "style_consultation",
                "complex_analysis": "complex_reasoning",
                "general_inquiry": "general_response"
            }
        )
        
        # Multi-step processing flow
        workflow.add_edge("decompose_query", "process_sub_query")
        workflow.add_conditional_edges(
            "process_sub_query",
            self.check_sub_queries_complete,
            {
                "continue": "process_sub_query",
                "synthesize": "synthesize_answers"
            }
        )
        
        # Finalization flow
        workflow.add_edge("medical_consultation", "finalize_response")
        workflow.add_edge("product_recommendation", "finalize_response")
        workflow.add_edge("technical_analysis", "finalize_response")
        workflow.add_edge("style_consultation", "finalize_response")
        workflow.add_edge("complex_reasoning", "finalize_response")
        workflow.add_edge("general_response", "finalize_response")
        workflow.add_edge("synthesize_answers", "finalize_response")
        
        # End states
        workflow.add_edge("finalize_response", END)
        workflow.add_edge("handle_error", END)
        
        return workflow
    
    def analyze_intent_node(self, state: WorkflowState) -> WorkflowState:
        """Advanced intent analysis with LLM"""
        try:
            print("ðŸ§  Analyzing query intent...")
            start_time = time.time()
            
            # Intent detection prompt
            intent_prompt = PromptTemplate(
                input_variables=["query"],
                template="""
PhÃ¢n tÃ­ch cÃ¢u há»i sau vá» máº¯t kÃ­nh vÃ  xÃ¡c Ä‘á»‹nh intent:

CÃ¢u há»i: {query}

HÃ£y phÃ¢n loáº¡i intent theo cÃ¡c categories:
1. medical_consultation: CÃ¢u há»i vá» sá»©c khá»e máº¯t, bá»‡nh lÃ½, Ä‘iá»u trá»‹
2. product_recommendation: TÃ¬m hiá»ƒu/so sÃ¡nh sáº£n pháº©m máº¯t kÃ­nh
3. technical_advice: ThÃ´ng tin ká»¹ thuáº­t vá» trÃ²ng kÃ­nh, cÃ´ng nghá»‡
4. style_consultation: TÆ° váº¥n phong cÃ¡ch, tháº©m má»¹
5. general_inquiry: CÃ¢u há»i chung chung
6. complex_analysis: YÃªu cáº§u phÃ¢n tÃ­ch phá»©c táº¡p, nhiá»u bÆ°á»›c

CÅ©ng Ä‘Ã¡nh giÃ¡:
- Äá»™ phá»©c táº¡p: simple/moderate/complex/expert
- CÃ³ cáº§n multi-step reasoning khÃ´ng?
- Confidence score (0-1)
- Key entities trong cÃ¢u há»i

Tráº£ vá» JSON format vá»›i cÃ¡c fields: intent_type, confidence, sub_intents, complexity_level, requires_multi_step, key_entities
"""
            )
            
            # Call LLM for intent detection
            prompt_formatted = intent_prompt.format(query=state["query"])
            messages = [SystemMessage(content="You are an expert intent classifier for eyewear domain."),
                       HumanMessage(content=prompt_formatted)]
            
            response = self.intent_llm.invoke(messages)
            
            # Parse response (simplified - in production use structured output)
            try:
                # Extract JSON from response (basic parsing)
                content = response.content
                if "```json" in content:
                    json_part = content.split("```json")[1].split("```")[0]
                elif "{" in content and "}" in content:
                    start_idx = content.find("{")
                    end_idx = content.rfind("}") + 1
                    json_part = content[start_idx:end_idx]
                else:
                    # Fallback
                    json_part = '{"intent_type": "general_inquiry", "confidence": 0.7, "sub_intents": [], "complexity_level": "moderate", "requires_multi_step": false, "key_entities": []}'
                
                intent_data = json.loads(json_part)
                intent = QueryIntent(**intent_data)
                
            except:
                # Fallback intent
                intent = QueryIntent(
                    intent_type="general_inquiry",
                    confidence=0.7,
                    complexity_level="moderate",
                    requires_multi_step=False
                )
            
            state["intent"] = intent
            state["step_history"] = ["analyze_intent"]
            state["processing_time"] = time.time() - start_time
            
            print(f"ðŸŽ¯ Intent detected: {intent.intent_type} (confidence: {intent.confidence:.2f})")
            print(f"ðŸ” Complexity: {intent.complexity_level}, Multi-step: {intent.requires_multi_step}")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["status"] = "error"
            print(f"âŒ Error in intent analysis: {e}")
            return state
    
    def plan_strategy_node(self, state: WorkflowState) -> WorkflowState:
        """Plan processing strategy based on intent analysis"""
        try:
            print("ðŸ“‹ Planning processing strategy...")
            
            intent = state["intent"]
            
            if intent.requires_multi_step or intent.complexity_level in ["complex", "expert"]:
                state["processing_strategy"] = "multi_step"
                print("ðŸ”„ Strategy: Multi-step reasoning")
            else:
                state["processing_strategy"] = "simple_retrieval"
                print("âž¡ï¸  Strategy: Simple retrieval")
            
            state["step_history"].append("plan_strategy")
            return state
            
        except Exception as e:
            state["processing_strategy"] = "error"
            state["error_count"] = state.get("error_count", 0) + 1
            return state
    
    def retrieve_context_node(self, state: WorkflowState) -> WorkflowState:
        """Enhanced document retrieval with dynamic parameters"""
        try:
            print("ðŸ” Retrieving relevant context...")
            
            # Create query embedding
            query_embedding = self.embedding_manager.embed_query(state["query"])
            state["query_embedding"] = query_embedding.tolist()
            
            # Dynamic retrieval parameters based on intent
            intent = state["intent"]
            if intent.complexity_level == "expert":
                top_k = min(15, Config.TOP_K_DOCUMENTS + 5)
            elif intent.complexity_level == "complex":
                top_k = min(12, Config.TOP_K_DOCUMENTS + 2)
            else:
                top_k = Config.TOP_K_DOCUMENTS
            
            # Retrieve documents
            retrieved_docs = self.qdrant_manager.search_similar_documents(
                query_embedding.tolist(), 
                top_k=top_k
            )
            
            state["retrieved_documents"] = retrieved_docs
            state["step_history"].append("retrieve_context")
            
            print(f"ðŸ“„ Retrieved {len(retrieved_docs)} documents")
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["status"] = "error"
            return state
    
    def filter_documents_node(self, state: WorkflowState) -> WorkflowState:
        """Intelligent document filtering using LLM"""
        try:
            print("ðŸŽ¯ Filtering documents by relevance...")
            
            intent = state["intent"]
            retrieved_docs = state["retrieved_documents"]
            
            if not retrieved_docs:
                state["filtered_documents"] = []
                return state
            
            # LLM-based relevance filtering
            filter_prompt = f"""
ÄÃ¡nh giÃ¡ relevance cá»§a documents sau cho cÃ¢u há»i: "{state['query']}"
Intent type: {intent.intent_type}
Key entities: {intent.key_entities}

Chá»‰ giá»¯ láº¡i documents thá»±c sá»± há»¯u Ã­ch cho intent nÃ y.
Tráº£ vá» danh sÃ¡ch index cá»§a documents relevance (tá»« 0 Ä‘áº¿n {len(retrieved_docs)-1}).
"""
            
            # Simplified filtering (can be enhanced with LLM scoring)
            filtered_docs = retrieved_docs[:8]  # Basic filtering
            
            state["filtered_documents"] = filtered_docs
            state["step_history"].append("filter_documents")
            
            print(f"âœ… Filtered to {len(filtered_docs)} most relevant documents")
            return state
            
        except Exception as e:
            state["filtered_documents"] = state.get("retrieved_documents", [])
            state["error_count"] = state.get("error_count", 0) + 1
            return state
    
    # Specialized processing nodes
    def medical_consultation_node(self, state: WorkflowState) -> WorkflowState:
        """Specialized medical consultation processing"""
        try:
            print("ðŸ¥ Processing medical consultation...")
            
            medical_prompt = """
Báº¡n lÃ  chuyÃªn gia nhÃ£n khoa vá»›i 20+ nÄƒm kinh nghiá»‡m. HÃ£y tÆ° váº¥n vá» váº¥n Ä‘á» máº¯t/kÃ­nh dá»±a trÃªn thÃ´ng tin sau:

Context: {context}
CÃ¢u há»i: {query}

HÆ°á»›ng dáº«n tráº£ lá»i:
1. PhÃ¢n tÃ­ch triá»‡u chá»©ng/váº¥n Ä‘á» náº¿u cÃ³
2. ÄÆ°a ra lá»i khuyÃªn chuyÃªn mÃ´n
3. LUÃ”N khuyáº¿n nghá»‹ thÄƒm khÃ¡m bÃ¡c sÄ© nhÃ£n khoa náº¿u liÃªn quan sá»©c khá»e
4. Cung cáº¥p thÃ´ng tin an toÃ n vÃ  cÃ³ cÄƒn cá»©
5. TrÃ¡nh cháº©n Ä‘oÃ¡n qua internet

LÆ°u Ã½: Æ¯u tiÃªn sá»± an toÃ n cá»§a bá»‡nh nhÃ¢n.
"""
            
            result = self.rag_agent.process_query(
                state["query"], 
                state["filtered_documents"],
                custom_prompt=medical_prompt
            )
            
            state["final_answer"] = result["answer"]
            state["sources"] = result.get("sources", [])
            state["confidence_score"] = 0.9  # High confidence for medical
            state["step_history"].append("medical_consultation")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["final_answer"] = "Xin lá»—i, tÃ´i gáº·p lá»—i khi xá»­ lÃ½ tÆ° váº¥n y táº¿. Vui lÃ²ng thÄƒm khÃ¡m bÃ¡c sÄ© nhÃ£n khoa."
            return state
    
    def product_recommendation_node(self, state: WorkflowState) -> WorkflowState:
        """Specialized product recommendation processing"""
        try:
            print("ðŸ›ï¸ Processing product recommendation...")
            
            product_prompt = """
Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n sáº£n pháº©m máº¯t kÃ­nh vá»›i kiáº¿n thá»©c sÃ¢u vá» cÃ¡c thÆ°Æ¡ng hiá»‡u vÃ  cÃ´ng nghá»‡.

Context: {context}
CÃ¢u há»i: {query}

HÃ£y Ä‘Æ°a ra khuyáº¿n nghá»‹ sáº£n pháº©m:
1. PhÃ¢n tÃ­ch nhu cáº§u cá»§a khÃ¡ch hÃ ng
2. So sÃ¡nh cÃ¡c lá»±a chá»n phÃ¹ há»£p
3. Giáº£i thÃ­ch Æ°u/nhÆ°á»£c Ä‘iá»ƒm tá»«ng loáº¡i
4. ÄÆ°a ra khuyáº¿n nghá»‹ cá»¥ thá»ƒ vá»›i lÃ½ do
5. Äá» xuáº¥t má»©c giÃ¡ vÃ  nÆ¡i mua náº¿u cÃ³
6. LÆ°u Ã½ vá» tÆ°Æ¡ng thÃ­ch vá»›i Ä‘áº·c Ä‘iá»ƒm cÃ¡ nhÃ¢n

Phong cÃ¡ch: ThÃ¢n thiá»‡n, chuyÃªn nghiá»‡p, dá»… hiá»ƒu.
"""
            
            result = self.rag_agent.process_query(
                state["query"],
                state["filtered_documents"], 
                custom_prompt=product_prompt
            )
            
            state["final_answer"] = result["answer"]
            state["sources"] = result.get("sources", [])
            state["confidence_score"] = 0.85
            state["step_history"].append("product_recommendation")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["final_answer"] = "Xin lá»—i, tÃ´i gáº·p lá»—i khi Ä‘Æ°a ra khuyáº¿n nghá»‹ sáº£n pháº©m."
            return state
    
    def technical_analysis_node(self, state: WorkflowState) -> WorkflowState:
        """Technical analysis processing"""
        try:
            print("ðŸ”¬ Processing technical analysis...")
            
            technical_prompt = """
Báº¡n lÃ  ká»¹ sÆ° quang há»c chuyÃªn vá» cÃ´ng nghá»‡ trÃ²ng kÃ­nh vÃ  gá»ng máº¯t kÃ­nh.

Context: {context}
CÃ¢u há»i: {query}

HÃ£y Ä‘Æ°a ra phÃ¢n tÃ­ch ká»¹ thuáº­t:
1. Giáº£i thÃ­ch cÃ¡c khÃ¡i niá»‡m/cÃ´ng nghá»‡ liÃªn quan
2. PhÃ¢n tÃ­ch Æ°u/nhÆ°á»£c Ä‘iá»ƒm ká»¹ thuáº­t
3. So sÃ¡nh vá»›i cÃ¡c cÃ´ng nghá»‡ khÃ¡c
4. ÄÆ°a ra Ä‘Ã¡nh giÃ¡ khÃ¡ch quan
5. Giáº£i thÃ­ch cÃ¡ch hoáº¡t Ä‘á»™ng náº¿u phÃ¹ há»£p
6. Äá» xuáº¥t á»©ng dá»¥ng thá»±c táº¿

Phong cÃ¡ch: ChÃ­nh xÃ¡c, khoa há»c, chi tiáº¿t nhÆ°ng dá»… hiá»ƒu.
"""
            
            result = self.rag_agent.process_query(
                state["query"],
                state["filtered_documents"],
                custom_prompt=technical_prompt
            )
            
            state["final_answer"] = result["answer"]
            state["sources"] = result.get("sources", [])
            state["confidence_score"] = 0.88
            state["step_history"].append("technical_analysis")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["final_answer"] = "Xin lá»—i, tÃ´i gáº·p lá»—i khi phÃ¢n tÃ­ch ká»¹ thuáº­t."
            return state
    
    def style_consultation_node(self, state: WorkflowState) -> WorkflowState:
        """Style consultation processing"""
        try:
            print("ðŸ‘“ Processing style consultation...")
            
            style_prompt = """
Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n phong cÃ¡ch vÃ  tháº©m má»¹ máº¯t kÃ­nh vá»›i kinh nghiá»‡m styling cho nhiá»u dÃ¡ng máº·t.

Context: {context}
CÃ¢u há»i: {query}

HÃ£y tÆ° váº¥n phong cÃ¡ch:
1. PhÃ¢n tÃ­ch Ä‘áº·c Ä‘iá»ƒm khuÃ´n máº·t/phong cÃ¡ch cÃ¡ nhÃ¢n
2. Khuyáº¿n nghá»‹ kiá»ƒu gá»ng phÃ¹ há»£p
3. TÆ° váº¥n vá» mÃ u sáº¯c vÃ  cháº¥t liá»‡u
4. ÄÆ°a ra tips phá»‘i Ä‘á»“ vá»›i máº¯t kÃ­nh
5. Gá»£i Ã½ xu hÆ°á»›ng thá»i trang máº¯t kÃ­nh
6. CÃ¢n nháº¯c vá» tÃ­nh thá»±c táº¿ vÃ  sá»­ dá»¥ng

Phong cÃ¡ch: ThÃ¢n thiá»‡n, sÃ¡ng táº¡o, cáº­p nháº­t xu hÆ°á»›ng.
"""
            
            result = self.rag_agent.process_query(
                state["query"],
                state["filtered_documents"],
                custom_prompt=style_prompt
            )
            
            state["final_answer"] = result["answer"]
            state["sources"] = result.get("sources", [])
            state["confidence_score"] = 0.82
            state["step_history"].append("style_consultation")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["final_answer"] = "Xin lá»—i, tÃ´i gáº·p lá»—i khi tÆ° váº¥n phong cÃ¡ch."
            return state
    
    def complex_reasoning_node(self, state: WorkflowState) -> WorkflowState:
        """Complex multi-faceted analysis"""
        try:
            print("ðŸ§© Processing complex analysis...")
            
            complex_prompt = """
Báº¡n lÃ  chuyÃªn gia tá»•ng há»£p vá»›i kiáº¿n thá»©c Ä‘a ngÃ nh vá» máº¯t kÃ­nh (y táº¿, ká»¹ thuáº­t, tháº©m má»¹, kinh táº¿).

Context: {context}
CÃ¢u há»i phá»©c táº¡p: {query}

HÃ£y Ä‘Æ°a ra phÃ¢n tÃ­ch toÃ n diá»‡n:
1. PhÃ¢n tÃ­ch Ä‘a chiá»u váº¥n Ä‘á»
2. Xem xÃ©t cÃ¡c yáº¿u tá»‘ liÃªn quan (sá»©c khá»e, kinh táº¿, tháº©m má»¹, ká»¹ thuáº­t)
3. ÄÆ°a ra pros/cons cá»§a cÃ¡c lá»±a chá»n
4. Khuyáº¿n nghá»‹ tá»‘i Æ°u dá»±a trÃªn trade-offs
5. Äá» xuáº¥t cÃ¡c bÆ°á»›c thá»±c hiá»‡n cá»¥ thá»ƒ
6. CÃ¢n nháº¯c rá»§i ro vÃ  lÆ°u Ã½

Phong cÃ¡ch: ToÃ n diá»‡n, cÃ¢n báº±ng, cÃ³ cáº¥u trÃºc rÃµ rÃ ng.
"""
            
            result = self.rag_agent.process_query(
                state["query"],
                state["filtered_documents"],
                custom_prompt=complex_prompt
            )
            
            state["final_answer"] = result["answer"]
            state["sources"] = result.get("sources", [])
            state["confidence_score"] = 0.87
            state["step_history"].append("complex_reasoning")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["final_answer"] = "Xin lá»—i, tÃ´i gáº·p lá»—i khi phÃ¢n tÃ­ch phá»©c táº¡p."
            return state
    
    def general_response_node(self, state: WorkflowState) -> WorkflowState:
        """General response processing"""
        try:
            print("ðŸ’¬ Processing general inquiry...")
            
            result = self.rag_agent.process_query(
                state["query"],
                state["filtered_documents"]
            )
            
            state["final_answer"] = result["answer"]
            state["sources"] = result.get("sources", [])
            state["confidence_score"] = 0.75
            state["step_history"].append("general_response")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["final_answer"] = "Xin lá»—i, tÃ´i gáº·p lá»—i khi xá»­ lÃ½ cÃ¢u há»i."
            return state
    
    # Multi-step processing nodes
    def decompose_query_node(self, state: WorkflowState) -> WorkflowState:
        """Decompose complex query into sub-queries"""
        try:
            print("ðŸ”„ Decomposing complex query...")
            
            # Use LLM to break down complex query
            decompose_prompt = f"""
PhÃ¢n tÃ¡ch cÃ¢u há»i phá»©c táº¡p sau thÃ nh cÃ¡c sub-queries Ä‘Æ¡n giáº£n hÆ¡n:

Query: {state['query']}
Intent: {state['intent'].intent_type}

Chia thÃ nh 2-4 cÃ¢u há»i con cÃ³ thá»ƒ tráº£ lá»i Ä‘á»™c láº­p:
1. [Sub-query 1]
2. [Sub-query 2]
...

Tráº£ vá» danh sÃ¡ch cÃ¡c sub-queries.
"""
            
            # Simplified decomposition (can enhance with LLM)
            sub_queries = [
                f"ThÃ´ng tin cÆ¡ báº£n vá» {' '.join(state['intent'].key_entities)}",
                f"Khuyáº¿n nghá»‹ cá»¥ thá»ƒ cho {state['query'][:50]}...",
                "LÆ°u Ã½ vÃ  cÃ¢n nháº¯c quan trá»ng"
            ]
            
            state["sub_queries"] = sub_queries
            state["current_step"] = 0
            state["intermediate_answers"] = []
            state["step_history"].append("decompose_query")
            
            print(f"ðŸ§© Decomposed into {len(sub_queries)} sub-queries")
            return state
            
        except Exception as e:
            state["processing_strategy"] = "error"
            state["error_count"] = state.get("error_count", 0) + 1
            return state
    
    def process_sub_query_node(self, state: WorkflowState) -> WorkflowState:
        """Process individual sub-query"""
        try:
            current_step = state["current_step"]
            sub_queries = state["sub_queries"]
            
            if current_step < len(sub_queries):
                sub_query = sub_queries[current_step]
                print(f"ðŸ” Processing sub-query {current_step + 1}: {sub_query}")
                
                # Process sub-query with relevant documents
                result = self.rag_agent.process_query(
                    sub_query,
                    state["filtered_documents"]
                )
                
                state["intermediate_answers"].append({
                    "sub_query": sub_query,
                    "answer": result["answer"],
                    "sources": result.get("sources", [])
                })
                
                state["current_step"] += 1
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            return state
    
    def synthesize_answers_node(self, state: WorkflowState) -> WorkflowState:
        """Synthesize multiple answers into comprehensive response"""
        try:
            print("ðŸ”— Synthesizing comprehensive answer...")
            
            intermediate_answers = state["intermediate_answers"]
            
            # Create synthesis prompt
            synthesis_content = "\n\n".join([
                f"Q: {ans['sub_query']}\nA: {ans['answer']}"
                for ans in intermediate_answers
            ])
            
            synthesis_prompt = f"""
Tá»•ng há»£p cÃ¡c cÃ¢u tráº£ lá»i sau thÃ nh má»™t pháº£n há»“i hoÃ n chá»‰nh cho cÃ¢u há»i gá»‘c:

CÃ¢u há»i gá»‘c: {state['query']}

CÃ¡c cÃ¢u tráº£ lá»i thÃ nh pháº§n:
{synthesis_content}

HÃ£y táº¡o má»™t cÃ¢u tráº£ lá»i tá»•ng há»£p:
1. CÃ³ cáº¥u trÃºc rÃµ rÃ ng
2. Loáº¡i bá» thÃ´ng tin trÃ¹ng láº·p
3. Káº¿t ná»‘i logic giá»¯a cÃ¡c pháº§n
4. ÄÆ°a ra káº¿t luáº­n tá»•ng thá»ƒ
5. Giá»¯ nguyÃªn cÃ¡c thÃ´ng tin quan trá»ng
"""
            
            # Use reasoning LLM for synthesis
            messages = [
                SystemMessage(content="You are an expert at synthesizing information."),
                HumanMessage(content=synthesis_prompt)
            ]
            
            response = self.reasoning_llm.invoke(messages)
            
            state["final_answer"] = response.content
            
            # Collect all sources
            all_sources = []
            for ans in intermediate_answers:
                all_sources.extend(ans.get("sources", []))
            state["sources"] = list(set(all_sources))  # Remove duplicates
            
            state["confidence_score"] = 0.85
            state["step_history"].append("synthesize_answers")
            
            return state
            
        except Exception as e:
            # Fallback to simple concatenation
            answers = [ans["answer"] for ans in state["intermediate_answers"]]
            state["final_answer"] = "\n\n".join(answers)
            state["error_count"] = state.get("error_count", 0) + 1
            return state
    
    def finalize_response_node(self, state: WorkflowState) -> WorkflowState:
        """Finalize and enhance the response"""
        try:
            print("âœ¨ Finalizing response...")
            
            # Add metadata and enhancements
            if not state.get("final_answer"):
                state["final_answer"] = "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y."
                state["confidence_score"] = 0.0
            
            # Ensure we have sources
            if not state.get("sources"):
                state["sources"] = []
            
            # Calculate total processing time
            total_time = time.time() - state.get("processing_time", time.time())
            state["processing_time"] = total_time
            
            state["status"] = "completed"
            state["step_history"].append("finalize_response")
            
            print(f"âœ… Response finalized in {total_time:.2f}s")
            return state
            
        except Exception as e:
            state["status"] = "error"
            state["error_count"] = state.get("error_count", 0) + 1
            return state
    
    def handle_error_node(self, state: WorkflowState) -> WorkflowState:
        """Enhanced error handling with recovery"""
        try:
            error_count = state.get("error_count", 0)
            
            if error_count < 3:  # Retry logic
                print(f"âš ï¸ Error occurred, attempting recovery (attempt {error_count + 1}/3)")
                state["final_answer"] = "Äang thá»­ láº¡i xá»­ lÃ½ cÃ¢u há»i..."
                state["retry_count"] = state.get("retry_count", 0) + 1
            else:
                print("âŒ Maximum errors reached, providing fallback response")
                state["final_answer"] = """
Xin lá»—i, tÃ´i gáº·p khÃ³ khÄƒn khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n. 

Vui lÃ²ng:
1. Thá»­ láº¡i vá»›i cÃ¢u há»i Ä‘Æ¡n giáº£n hÆ¡n
2. Kiá»ƒm tra káº¿t ná»‘i internet
3. LiÃªn há»‡ há»— trá»£ ká»¹ thuáº­t náº¿u váº¥n Ä‘á» tiáº¿p tá»¥c

Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng dá»‹ch vá»¥ tÆ° váº¥n máº¯t kÃ­nh!
"""
            
            state["status"] = "error_handled"
            state["confidence_score"] = 0.1
            state["sources"] = []
            
            return state
            
        except Exception as e:
            state["final_answer"] = "Há»‡ thá»‘ng gáº·p lá»—i nghiÃªm trá»ng. Vui lÃ²ng thá»­ láº¡i sau."
            state["status"] = "critical_error"
            return state
    
    # Routing functions
    def route_by_strategy(self, state: WorkflowState) -> str:
        """Route based on processing strategy"""
        strategy = state.get("processing_strategy", "simple_retrieval")
        
        if strategy == "error":
            return "error"
        elif strategy == "multi_step":
            return "multi_step"
        else:
            return "simple_retrieval"
    
    def route_by_intent(self, state: WorkflowState) -> str:
        """Route based on detected intent"""
        intent = state.get("intent")
        if not intent:
            return "general_inquiry"
        
        return intent.intent_type
    
    def check_sub_queries_complete(self, state: WorkflowState) -> str:
        """Check if all sub-queries are processed"""
        current_step = state.get("current_step", 0)
        total_steps = len(state.get("sub_queries", []))
        
        if current_step < total_steps:
            return "continue"
        else:
            return "synthesize"
    
    # Main execution methods
    def invoke(self, query: str, user_context: Dict = None, **kwargs) -> Dict:
        """
        Main invoke method for the enhanced workflow
        
        Args:
            query: User question
            user_context: User profile/preferences/history
            **kwargs: Additional parameters
        
        Returns:
            Comprehensive result dictionary
        """
        try:
            # Initialize state
            initial_state = WorkflowState(
                query=query,
                user_context=user_context or {},
                reasoning_steps=[],
                current_step=0,
                sub_queries=[],
                retrieved_documents=[],
                filtered_documents=[],
                intermediate_answers=[],
                messages=[],
                step_history=[],
                error_count=0,
                retry_count=0,
                processing_time=time.time(),
                tokens_used=0,
                status="processing"
            )
            
            # Run workflow
            final_state = self.compiled_workflow.invoke(initial_state)
            
            # Format response
            return {
                "query": query,
                "answer": final_state.get("final_answer", ""),
                "sources": final_state.get("sources", []),
                "intent": final_state.get("intent").dict() if final_state.get("intent") else None,
                "confidence_score": final_state.get("confidence_score", 0.0),
                "processing_time": final_state.get("processing_time", 0.0),
                "step_history": final_state.get("step_history", []),
                "status": final_state.get("status", "unknown"),
                "metadata": {
                    "workflow_type": "enhanced_langgraph",
                    "error_count": final_state.get("error_count", 0),
                    "retry_count": final_state.get("retry_count", 0),
                    "reasoning_steps": len(final_state.get("reasoning_steps", [])),
                    "documents_retrieved": len(final_state.get("retrieved_documents", [])),
                    "documents_filtered": len(final_state.get("filtered_documents", []))
                }
            }
            
        except Exception as e:
            return {
                "query": query,
                "answer": f"Lá»—i nghiÃªm trá»ng trong workflow: {str(e)}",
                "sources": [],
                "intent": None,
                "confidence_score": 0.0,
                "processing_time": 0.0,
                "step_history": ["error"],
                "status": "critical_error",
                "error": str(e),
                "metadata": {"workflow_type": "enhanced_langgraph"}
            }

def create_enhanced_workflow() -> EnhancedEyewearWorkflow:
    """Factory function to create enhanced workflow"""
    return EnhancedEyewearWorkflow()

def get_enhanced_workflow() -> EnhancedEyewearWorkflow:
    """Get singleton instance of enhanced workflow"""
    global _enhanced_workflow_instance
    if '_enhanced_workflow_instance' not in globals():
        _enhanced_workflow_instance = create_enhanced_workflow()
    return _enhanced_workflow_instance 