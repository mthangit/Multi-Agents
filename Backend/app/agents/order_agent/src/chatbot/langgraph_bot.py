from langgraph.graph import StateGraph
from langchain_google_genai import ChatGoogleGenerativeAI
from typing import Annotated, TypedDict, AsyncIterator, Dict, Any
import operator
import asyncio
import logging
import json
import os
from datetime import datetime
from src.config import settings

# Thiáº¿t láº­p logging
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                   handlers=[logging.StreamHandler()])
logger = logging.getLogger("chatbot_debug")

# Import cÃ¡c node tools
from src.chatbot.nodes.product_tools import (
    find_product_by_id_node,
    find_product_by_name_node
)
from src.chatbot.nodes.cart_tools import (
    check_stock_node,
    add_to_cart_node,
    view_cart_node,
    clear_cart_node
)
from src.chatbot.nodes.order_tools import (
    start_order_process_node,
    collect_order_info_node,
    create_order_node
)
from src.chatbot.nodes.intent_nodes import (
    intent_classification_node,
    parameter_extraction_node,
    generate_response_node
)
from src.chatbot.nodes.conversation_node import (
    check_conversation_stage_node,
    generate_question_node
)
from src.chatbot.state import ChatState, initial_state

# Khá»Ÿi táº¡o LLM Gemini tá»« settings
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key=settings.GEMINI_API_KEY)

# Streaming version cá»§a LLM
streaming_llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash", 
    google_api_key=settings.GEMINI_API_KEY,
    streaming=True
)

def save_debug_state(state, node_name):
    """LÆ°u state táº¡i má»—i bÆ°á»›c vÃ o file Ä‘á»ƒ phÃ¢n tÃ­ch"""
    debug_dir = os.path.join(os.getcwd(), "debug_logs")
    os.makedirs(debug_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    filename = f"{timestamp}_{node_name}.json"
    filepath = os.path.join(debug_dir, filename)
    
    # Convert state to serializable format
    serializable_state = {}
    for key, value in state.items():
        if key == "message" or key == "intent" or key == "response" or key == "conversation_stage":
            serializable_state[key] = value
        else:
            try:
                # Attempt to convert to JSON-serializable format
                json.dumps({key: value})
                serializable_state[key] = value
            except (TypeError, OverflowError):
                # If not serializable, convert to string
                serializable_state[key] = str(value)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(serializable_state, f, ensure_ascii=False, indent=2)
    
    logger.info(f"Saved debug state for node {node_name} to {filepath}")
    return state

def debug_state(state, node_name):
    """Helper function Ä‘á»ƒ log state táº¡i má»—i node"""
    logger.info(f"Executing node: {node_name}")
    logger.info(f"Current intent: {state.get('intent')}")
    logger.info(f"Current parameters: {state.get('parameters')}")
    logger.info(f"Current conversation stage: {state.get('conversation_stage')}")
    
    # Save state to file for debugging
    save_debug_state(state, node_name)
    
    return state

def welcome_node(state):
    """Node chÃ o má»«ng, sáº½ Ä‘Æ°á»£c gá»i khi báº¯t Ä‘áº§u há»™i thoáº¡i má»›i"""
    debug_state(state, "welcome_node")
    return {"response": "Xin chÃ o! TÃ´i lÃ  trá»£ lÃ½ áº£o cá»§a há»‡ thá»‘ng quáº£n lÃ½ Ä‘Æ¡n hÃ ng. TÃ´i cÃ³ thá»ƒ giÃºp báº¡n tÃ¬m sáº£n pháº©m, thÃªm vÃ o giá» hÃ ng, Ä‘áº·t hÃ ng hoáº·c kiá»ƒm tra tráº¡ng thÃ¡i Ä‘Æ¡n hÃ ng. Báº¡n cáº§n giÃºp gÃ¬?"}

def help_node(state):
    """Node trá»£ giÃºp, cung cáº¥p hÆ°á»›ng dáº«n vá» cÃ¡c tÃ­nh nÄƒng"""
    debug_state(state, "help_node")
    help_text = """
    TÃ´i cÃ³ thá»ƒ giÃºp báº¡n vá»›i cÃ¡c tÃ¡c vá»¥ sau:
    1. TÃ¬m sáº£n pháº©m theo tÃªn (vÃ­ dá»¥: "TÃ¬m sáº£n pháº©m tÃªn lÃ  iPhone")
    2. TÃ¬m sáº£n pháº©m theo ID (vÃ­ dá»¥: "Kiá»ƒm tra sáº£n pháº©m cÃ³ ID 123")
    3. Kiá»ƒm tra tá»“n kho (vÃ­ dá»¥: "Sáº£n pháº©m ID 123 cÃ²n hÃ ng khÃ´ng?")
    4. ThÃªm vÃ o giá» hÃ ng (vÃ­ dá»¥: "ThÃªm 2 sáº£n pháº©m ID 123 vÃ o giá»")
    5. Xem giá» hÃ ng (vÃ­ dá»¥: "Xem giá» hÃ ng cá»§a tÃ´i")
    6. Äáº·t hÃ ng (vÃ­ dá»¥: "TÃ´i muá»‘n Ä‘áº·t hÃ ng")
    7. Kiá»ƒm tra Ä‘Æ¡n hÃ ng (vÃ­ dá»¥: "Kiá»ƒm tra Ä‘Æ¡n hÃ ng sá»‘ 789")
    
    Báº¡n cáº§n giÃºp Ä‘á»¡ gÃ¬ áº¡?
    """
    return {"response": help_text}

def unknown_node(state):
    """Node xá»­ lÃ½ khi khÃ´ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c intent"""
    debug_state(state, "unknown_node")
    return {"response": "Xin lá»—i, tÃ´i khÃ´ng hiá»ƒu yÃªu cáº§u cá»§a báº¡n. Báº¡n cÃ³ thá»ƒ nÃ³i rÃµ hÆ¡n hoáº·c gÃµ 'help' Ä‘á»ƒ xem hÆ°á»›ng dáº«n."}

def get_order_by_id_node(state):
    """Node láº¥y thÃ´ng tin Ä‘Æ¡n hÃ ng theo ID"""
    debug_state(state, "get_order_by_id_node")
    parameters = state.get("parameters", {})
    order_id = parameters.get("order_id")
    
    if not order_id:
        return {"error": "KhÃ´ng tÃ¬m tháº¥y ID Ä‘Æ¡n hÃ ng"}
    
    from src.database.queries.order import OrderQuery
    order = OrderQuery().get_order_by_id(order_id)
    
    if not order:
        return {"error": f"KhÃ´ng tÃ¬m tháº¥y Ä‘Æ¡n hÃ ng vá»›i ID {order_id}"}
    
    return {"order": order}

def route_by_intent(state: ChatState):
    """XÃ¡c Ä‘á»‹nh node tiáº¿p theo dá»±a trÃªn intent hoáº·c conversation_stage"""
    debug_state(state, "route_by_intent")
    logger.info(f"Routing based on intent: {state.get('intent')} and conversation stage: {state.get('conversation_stage')}")
    
    # Náº¿u Ä‘ang trong má»™t cuá»™c trÃ² chuyá»‡n, Æ°u tiÃªn xá»­ lÃ½ theo stage
    conversation_stage = state.get("conversation_stage")
    if conversation_stage:
        if conversation_stage == "collecting_info":
            return "collect_order_info"
        elif conversation_stage == "confirm_order":
            return "create_order"
    
    # Náº¿u khÃ´ng, xá»­ lÃ½ theo intent
    intent = state.get("intent")
    if intent == "greet":
        return "welcome"
    elif intent == "help":
        return "help"
    elif intent == "unknown":
        return "unknown"
    elif intent == "collecting_order_info":
        return "collect_order_info"
    elif intent in ["find_product_by_name", "find_product_by_id", "check_stock", 
                   "add_to_cart", "view_cart", "clear_cart", "start_order", 
                   "get_order_by_id"]:
        return "parameter_extraction"
    else:
        return "unknown"

def route_to_tool(state: ChatState):
    """XÃ¡c Ä‘á»‹nh tool node dá»±a trÃªn intent sau khi Ä‘Ã£ cÃ³ parameters"""
    debug_state(state, "route_to_tool")
    intent = state.get("intent")
    logger.info(f"Routing to tool for intent: {intent}")
    tool_mapping = {
        "find_product_by_name": "find_product_by_name",
        "find_product_by_id": "find_product_by_id",
        "check_stock": "check_stock",
        "add_to_cart": "check_stock",  # Kiá»ƒm tra tá»“n kho trÆ°á»›c khi thÃªm vÃ o giá»
        "view_cart": "view_cart",
        "clear_cart": "clear_cart",
        "start_order": "start_order_process",
        "get_order_by_id": "get_order_by_id"
    }
    
    return tool_mapping.get(intent, "unknown")

def post_check_stock_router(state: ChatState):
    """Router sau khi kiá»ƒm tra tá»“n kho"""
    debug_state(state, "post_check_stock_router")
    intent = state.get("intent")
    error = state.get("error")
    
    if error:
        return "generate_response"
    
    if intent == "add_to_cart":
        return "add_to_cart"
    else:
        return "generate_response"

# Khá»Ÿi táº¡o LangGraph
class ChatbotGraph:
    def __init__(self):
        # Táº¡o state graph tá»« ChatState
        self.graph = StateGraph(ChatState)
        
        # ThÃªm cÃ¡c node
        # - Basic nodes
        self.graph.add_node("welcome", welcome_node)
        self.graph.add_node("help", help_node)
        self.graph.add_node("unknown", unknown_node)
        self.graph.add_node("intent_classification", intent_classification_node)
        self.graph.add_node("parameter_extraction", parameter_extraction_node)
        self.graph.add_node("generate_response", generate_response_node)
        
        # - Conversation management
        self.graph.add_node("check_conversation_stage", check_conversation_stage_node)
        self.graph.add_node("generate_question", generate_question_node)
        
        # - Product nodes
        self.graph.add_node("find_product_by_id", find_product_by_id_node)
        self.graph.add_node("find_product_by_name", find_product_by_name_node)
        self.graph.add_node("check_stock", check_stock_node)
        
        # - Cart nodes
        self.graph.add_node("add_to_cart", add_to_cart_node)
        self.graph.add_node("view_cart", view_cart_node)
        self.graph.add_node("clear_cart", clear_cart_node)
        
        # - Order nodes
        self.graph.add_node("start_order_process", start_order_process_node)
        self.graph.add_node("collect_order_info", collect_order_info_node)
        self.graph.add_node("create_order", create_order_node)
        self.graph.add_node("get_order_by_id", get_order_by_id_node)

        # Thiáº¿t láº­p entry point
        self.graph.set_entry_point("check_conversation_stage")

        # Thiáº¿t láº­p edges vÃ  conditional routing
        # - Main flow
        self.graph.add_edge("check_conversation_stage", "intent_classification")
        self.graph.add_conditional_edges("intent_classification", route_by_intent)
        self.graph.add_conditional_edges("parameter_extraction", route_to_tool)
        
        # - Product & stock flow
        self.graph.add_conditional_edges("check_stock", post_check_stock_router)
        
        # - Collection flow
        self.graph.add_edge("collect_order_info", "generate_question")
        
        # - Routing to generate_response
        tools = ["find_product_by_id", "find_product_by_name", "add_to_cart", 
                "view_cart", "clear_cart", "start_order_process", 
                "get_order_by_id", "create_order"]
                
        for tool in tools:
            self.graph.add_edge(tool, "generate_response")
            
        self.graph.add_edge("welcome", "generate_response")
        self.graph.add_edge("help", "generate_response")
        self.graph.add_edge("unknown", "generate_response")
        self.graph.add_edge("generate_question", "generate_response")

        # Compile graph
        self.app = self.graph.compile()

    def process_message(self, message: str, session_id: str = None) -> str:
        """Process user message and return response"""
        # Create initial state with user message
        state = initial_state()
        state["message"] = message
        state["user_session_id"] = session_id
        
        logger.info(f"Processing message: {message}")
        logger.info(f"Session ID: {session_id}")
        
        # Save initial state
        save_debug_state(state, "initial_state")
        
        # Run the graph
        result = self.app.invoke(state)
        
        # Log final state
        logger.info(f"Final state: intent={result.get('intent')}, response={result.get('response')}")
        
        # Save final state
        save_debug_state(result, "final_state")
        
        # Return the response
        return result["response"]
        
    async def process_message_streaming(self, message: str, session_id: str = None) -> AsyncIterator[str]:
        """Process user message vÃ  tráº£ vá» response theo kiá»ƒu streaming thá»±c sá»±"""
        logger.info(f"Processing streaming message: {message}")
        
        # Create initial state with user message
        state = initial_state()
        state["message"] = message
        state["user_session_id"] = session_id
        
        logger.info(f"Session ID: {session_id}")
        save_debug_state(state, "initial_state_streaming")
        
        # Yield initial processing message
        yield "Äang xá»­ lÃ½"
        await asyncio.sleep(0.1)
        yield "..."
        await asyncio.sleep(0.1)
        
        try:
            # Run the graph
            result = self.app.invoke(state)
            
            # Log final state
            logger.info(f"Final streaming state: intent={result.get('intent')}, response={result.get('response')}")
            save_debug_state(result, "final_state_streaming")
            
            response = result.get("response", "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ xá»­ lÃ½ yÃªu cáº§u nÃ y.")
            
            # Clear the processing message
            yield "\rğŸ¤– Bot: "
            await asyncio.sleep(0.05)
            
            # Stream the actual response word by word
            words = response.split()
            for i, word in enumerate(words):
                yield word
                if i < len(words) - 1:  # KhÃ´ng thÃªm space sau tá»« cuá»‘i
                    yield " "
                # Delay ngáº«u nhiÃªn Ä‘á»ƒ táº¡o hiá»‡u á»©ng typing tá»± nhiÃªn
                await asyncio.sleep(0.03 + (len(word) * 0.01))
                
        except Exception as e:
            logger.error(f"Error in streaming processing: {str(e)}")
            yield "\rğŸ¤– Bot: Xin lá»—i, Ä‘Ã£ xáº£y ra lá»—i khi xá»­ lÃ½ yÃªu cáº§u cá»§a báº¡n."