from langgraph.graph import StateGraph
from langchain_google_genai import ChatGoogleGenerativeAI
from typing import Annotated, TypedDict, AsyncIterator, Dict, Any
import operator
import asyncio
import logging
import json
import os
from datetime import datetime
from src.config import settings

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                   handlers=[logging.StreamHandler()])
logger = logging.getLogger("chatbot_debug")

# Import c√°c node tools
from src.chatbot.nodes.product_tools import (
    find_product_by_id_node,
    find_product_by_name_node
)
from src.chatbot.nodes.cart_tools import (
    check_stock_node,
    add_to_cart_node,
    view_cart_node,
    clear_cart_node
)
from src.chatbot.nodes.order_tools import (
    start_order_process_node,
    collect_order_info_node,
    create_order_node
)
from src.chatbot.nodes.intent_nodes import (
    intent_classification_node,
    parameter_extraction_node,
    generate_response_node
)
from src.chatbot.nodes.conversation_node import (
    check_conversation_stage_node,
    generate_question_node
)
from src.chatbot.nodes.memory_node import (
    load_session_node,
    update_memory_node,
    save_session_node,
    provide_context_node
)
from src.chatbot.state import ChatState, initial_state

# Kh·ªüi t·∫°o LLM Gemini t·ª´ settings
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key=settings.GEMINI_API_KEY)

# Streaming version c·ªßa LLM
streaming_llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash", 
    google_api_key=settings.GEMINI_API_KEY,
    streaming=True
)

def save_debug_state(state, node_name):
    """L∆∞u state t·∫°i m·ªói b∆∞·ªõc v√†o file ƒë·ªÉ ph√¢n t√≠ch"""
    debug_dir = os.path.join(os.getcwd(), "debug_logs")
    os.makedirs(debug_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    filename = f"{timestamp}_{node_name}.json"
    filepath = os.path.join(debug_dir, filename)
    
    # Convert state to serializable format
    serializable_state = {}
    for key, value in state.items():
        if key == "message" or key == "intent" or key == "response" or key == "conversation_stage":
            serializable_state[key] = value
        else:
            try:
                # Attempt to convert to JSON-serializable format
                json.dumps({key: value})
                serializable_state[key] = value
            except (TypeError, OverflowError):
                # If not serializable, convert to string
                serializable_state[key] = str(value)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(serializable_state, f, ensure_ascii=False, indent=2)
    
    logger.info(f"Saved debug state for node {node_name} to {filepath}")
    return state

def debug_state(state, node_name):
    """Helper function ƒë·ªÉ log state t·∫°i m·ªói node"""
    logger.info(f"Executing node: {node_name}")
    logger.info(f"Current intent: {state.get('intent')}")
    logger.info(f"Current parameters: {state.get('parameters')}")
    logger.info(f"Current conversation stage: {state.get('conversation_stage')}")
    
    # Save state to file for debugging
    # save_debug_state(state, node_name)
    
    return state

def welcome_node(state):
    """Node ch√†o m·ª´ng, s·∫Ω ƒë∆∞·ª£c g·ªçi khi b·∫Øt ƒë·∫ßu h·ªôi tho·∫°i m·ªõi"""
    debug_state(state, "welcome_node")
    return {"response": "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω ·∫£o c·ªßa h·ªá th·ªëng qu·∫£n l√Ω ƒë∆°n h√†ng. T√¥i c√≥ th·ªÉ gi√∫p b·∫°n t√¨m s·∫£n ph·∫©m, th√™m v√†o gi·ªè h√†ng, ƒë·∫∑t h√†ng ho·∫∑c ki·ªÉm tra tr·∫°ng th√°i ƒë∆°n h√†ng. B·∫°n c·∫ßn gi√∫p g√¨?"}

def help_node(state):
    """Node tr·ª£ gi√∫p, cung c·∫•p h∆∞·ªõng d·∫´n v·ªÅ c√°c t√≠nh nƒÉng"""
    debug_state(state, "help_node")
    help_text = """
    T√¥i c√≥ th·ªÉ gi√∫p b·∫°n v·ªõi c√°c t√°c v·ª• sau:
    1. T√¨m s·∫£n ph·∫©m theo t√™n (v√≠ d·ª•: "T√¨m s·∫£n ph·∫©m t√™n l√† iPhone")
    2. T√¨m s·∫£n ph·∫©m theo ID (v√≠ d·ª•: "Ki·ªÉm tra s·∫£n ph·∫©m c√≥ ID 123")
    3. Ki·ªÉm tra t·ªìn kho (v√≠ d·ª•: "S·∫£n ph·∫©m ID 123 c√≤n h√†ng kh√¥ng?")
    4. Th√™m v√†o gi·ªè h√†ng (v√≠ d·ª•: "Th√™m 2 s·∫£n ph·∫©m ID 123 v√†o gi·ªè")
    5. Xem gi·ªè h√†ng (v√≠ d·ª•: "Xem gi·ªè h√†ng c·ªßa t√¥i")
    6. ƒê·∫∑t h√†ng (v√≠ d·ª•: "T√¥i mu·ªën ƒë·∫∑t h√†ng")
    7. Ki·ªÉm tra ƒë∆°n h√†ng (v√≠ d·ª•: "Ki·ªÉm tra ƒë∆°n h√†ng s·ªë 789")
    
    B·∫°n c·∫ßn gi√∫p ƒë·ª° g√¨ ·∫°?
    """
    return {"response": help_text}

def unknown_node(state):
    """Node x·ª≠ l√Ω khi kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c intent"""
    debug_state(state, "unknown_node")
    return {"response": "Xin l·ªói, t√¥i kh√¥ng hi·ªÉu y√™u c·∫ßu c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ n√≥i r√µ h∆°n ho·∫∑c g√µ 'help' ƒë·ªÉ xem h∆∞·ªõng d·∫´n."}

def get_order_by_id_node(state):
    """Node l·∫•y th√¥ng tin ƒë∆°n h√†ng theo ID"""
    debug_state(state, "get_order_by_id_node")
    parameters = state.get("parameters", {})
    order_id = parameters.get("order_id")
    
    if not order_id:
        return {"error": "Kh√¥ng t√¨m th·∫•y ID ƒë∆°n h√†ng"}
    
    from src.database.queries.order import OrderQuery
    order = OrderQuery().get_order_by_id(order_id)
    
    if not order:
        return {"error": f"Kh√¥ng t√¨m th·∫•y ƒë∆°n h√†ng v·ªõi ID {order_id}"}
    
    return {"order": order}

def route_by_intent(state: ChatState):
    """X√°c ƒë·ªãnh node ti·∫øp theo d·ª±a tr√™n intent ho·∫∑c conversation_stage"""
    debug_state(state, "route_by_intent")
    logger.info(f"Routing based on intent: {state.get('intent')} and conversation stage: {state.get('conversation_stage')}")
    
    # N·∫øu ƒëang trong m·ªôt cu·ªôc tr√≤ chuy·ªán, ∆∞u ti√™n x·ª≠ l√Ω theo stage
    conversation_stage = state.get("conversation_stage")
    if conversation_stage:
        if conversation_stage == "collecting_info":
            return "collect_order_info"
        elif conversation_stage == "confirm_order":
            return "create_order"
    
    # N·∫øu kh√¥ng, x·ª≠ l√Ω theo intent
    intent = state.get("intent")
    if intent == "greet":
        return "welcome"
    elif intent == "help":
        return "help"
    elif intent == "unknown":
        return "unknown"
    elif intent == "collecting_order_info":
        return "collect_order_info"
    elif intent in ["find_product_by_name", "find_product_by_id", "check_stock", 
                   "add_to_cart", "view_cart", "clear_cart", "start_order", 
                   "get_order_by_id"]:
        return "parameter_extraction"
    else:
        return "unknown"

def route_to_tool(state: ChatState):
    """X√°c ƒë·ªãnh tool node d·ª±a tr√™n intent sau khi ƒë√£ c√≥ parameters"""
    debug_state(state, "route_to_tool")
    intent = state.get("intent")
    logger.info(f"Routing to tool for intent: {intent}")
    tool_mapping = {
        "find_product_by_name": "find_product_by_name",
        "find_product_by_id": "find_product_by_id",
        "check_stock": "check_stock",
        "add_to_cart": "check_stock",  # Ki·ªÉm tra t·ªìn kho tr∆∞·ªõc khi th√™m v√†o gi·ªè
        "view_cart": "view_cart",
        "clear_cart": "clear_cart",
        "start_order": "start_order_process",
        "get_order_by_id": "get_order_by_id"
    }
    
    return tool_mapping.get(intent, "unknown")

def post_check_stock_router(state: ChatState):
    """Router sau khi ki·ªÉm tra t·ªìn kho"""
    debug_state(state, "post_check_stock_router")
    intent = state.get("intent")
    error = state.get("error")
    
    if error:
        return "generate_response"
    
    if intent == "add_to_cart":
        return "add_to_cart"
    else:
        return "generate_response"

# Kh·ªüi t·∫°o LangGraph
class ChatbotGraph:
    def __init__(self):
        # T·∫°o state graph t·ª´ ChatState
        self.graph = StateGraph(ChatState)
        
        # Th√™m c√°c node
        # - Memory nodes
        self.graph.add_node("load_session", load_session_node)
        self.graph.add_node("provide_context", provide_context_node)
        self.graph.add_node("update_memory", update_memory_node)
        self.graph.add_node("save_session", save_session_node)
        
        # - Basic nodes
        self.graph.add_node("welcome", welcome_node)
        self.graph.add_node("help", help_node)
        self.graph.add_node("unknown", unknown_node)
        self.graph.add_node("intent_classification", intent_classification_node)
        self.graph.add_node("parameter_extraction", parameter_extraction_node)
        self.graph.add_node("generate_response", generate_response_node)
        
        # - Conversation management
        self.graph.add_node("check_conversation_stage", check_conversation_stage_node)
        self.graph.add_node("generate_question", generate_question_node)
        
        # - Product nodes
        self.graph.add_node("find_product_by_id", find_product_by_id_node)
        self.graph.add_node("find_product_by_name", find_product_by_name_node)
        self.graph.add_node("check_stock", check_stock_node)
        
        # - Cart nodes
        self.graph.add_node("add_to_cart", add_to_cart_node)
        self.graph.add_node("view_cart", view_cart_node)
        self.graph.add_node("clear_cart", clear_cart_node)
        
        # - Order nodes
        self.graph.add_node("start_order_process", start_order_process_node)
        self.graph.add_node("collect_order_info", collect_order_info_node)
        self.graph.add_node("create_order", create_order_node)
        self.graph.add_node("get_order_by_id", get_order_by_id_node)

        # Thi·∫øt l·∫≠p entry point v·ªõi memory
        self.graph.set_entry_point("load_session")

        # Thi·∫øt l·∫≠p edges v√† conditional routing
        # - Memory flow
        self.graph.add_edge("load_session", "provide_context")
        self.graph.add_edge("provide_context", "check_conversation_stage")
        
        # - Main flow
        self.graph.add_edge("check_conversation_stage", "intent_classification")
        self.graph.add_conditional_edges("intent_classification", route_by_intent)
        self.graph.add_conditional_edges("parameter_extraction", route_to_tool)
        
        # - Product & stock flow
        self.graph.add_conditional_edges("check_stock", post_check_stock_router)
        
        # - Collection flow
        self.graph.add_edge("collect_order_info", "generate_question")
        
        # - Routing to update_memory and then generate_response
        tools = ["find_product_by_id", "find_product_by_name", "add_to_cart", 
                "view_cart", "clear_cart", "start_order_process", 
                "get_order_by_id", "create_order"]
                
        for tool in tools:
            self.graph.add_edge(tool, "update_memory")
            
        self.graph.add_edge("welcome", "update_memory")
        self.graph.add_edge("help", "update_memory")
        self.graph.add_edge("unknown", "update_memory")
        self.graph.add_edge("generate_question", "update_memory")
        
        # - From update_memory to generate_response and then save_session
        self.graph.add_edge("update_memory", "generate_response")
        self.graph.add_edge("generate_response", "save_session")

        # Compile graph
        self.app = self.graph.compile()

    def process_message(self, message: str, session_id: str = None) -> str:
        """Process user message and return response with memory"""
        import uuid
        
        # Generate session ID if not provided
        if not session_id:
            session_id = str(uuid.uuid4())
            
        # Create initial state with user message
        state = initial_state()
        state["message"] = message
        state["user_session_id"] = session_id
                
        # Save initial state
        # save_debug_state(state, "initial_state")
        
        # Run the graph v·ªõi memory support
        result = self.app.invoke(state)
                
        # Save final state
        # save_debug_state(result, "final_state")
        
        # Return the response
        return result["response"]
        
    async def process_message_streaming(self, message: str, session_id: str = None) -> AsyncIterator[str]:
        """Process user message v√† tr·∫£ v·ªÅ response theo ki·ªÉu streaming v·ªõi memory"""
        import uuid
        
        # Generate session ID if not provided
        if not session_id:
            session_id = str(uuid.uuid4())
        
        # Create initial state with user message
        state = initial_state()
        state["message"] = message
        state["user_session_id"] = session_id
        
        # save_debug_state(state, "initial_state_streaming")
        await asyncio.sleep(0.1)
        
        try:
            # Run the graph v·ªõi memory
            result = self.app.invoke(state)
            
            # Log final state
            # save_debug_state(result, "final_state_streaming")
            
            response = result.get("response", "Xin l·ªói, t√¥i kh√¥ng th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu n√†y.")
            
            # Clear the processing message and show typing indicator
            await asyncio.sleep(0.1)
            yield "."
            await asyncio.sleep(0.1)
            yield "."
            await asyncio.sleep(0.1)
            yield "."
            await asyncio.sleep(0.1)
            
            yield "\rü§ñ Bot: "
            await asyncio.sleep(0.05)
            
            # Stream the actual response word by word
            words = response.split()
            for i, word in enumerate(words):
                yield word
                if i < len(words) - 1:  # Kh√¥ng th√™m space sau t·ª´ cu·ªëi
                    yield " "
                # Delay ng·∫´u nhi√™n ƒë·ªÉ t·∫°o hi·ªáu ·ª©ng typing t·ª± nhi√™n
                await asyncio.sleep(0.03 + (len(word) * 0.01))
                
        except Exception as e:
            logger.error(f"Error in streaming processing: {str(e)}")
            yield "\r‚ùå Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n."