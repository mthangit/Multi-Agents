from typing import List, Dict, Optional, Any
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage, SystemMessage
from config import Config, EYEWEAR_KEYWORDS
import re

class RAGAgent:
    def __init__(self):
        """
        Kh·ªüi t·∫°o RAG Agent v·ªõi Google Gemini cho domain m·∫Øt k√≠nh
        """
        print(f"ü§ñ ƒêang kh·ªüi t·∫°o RAG Agent cho domain: {Config.DOMAIN}")
        print(f"ü§ñ ƒêang kh·ªüi t·∫°o Gemini model: {Config.GEMINI_MODEL}")
        
        self.llm = ChatGoogleGenerativeAI(
            model=Config.GEMINI_MODEL,
            temperature=Config.GEMINI_TEMPERATURE,
            max_output_tokens=Config.GEMINI_MAX_OUTPUT_TOKENS,
            google_api_key=Config.GOOGLE_API_KEY
        )
        
        print("‚úÖ RAG Agent ƒë√£ s·∫µn s√†ng!")
    
    def detect_query_intent(self, query: str) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch intent c·ªßa c√¢u h·ªèi v·ªÅ m·∫Øt k√≠nh
        """
        intent_info = {
            "query_type": "general",
            "vision_condition": None,
            "product_interest": None,
            "style_preference": None,
            "technical_level": "basic"
        }
        
        query_lower = query.lower()
        
        # Detect vision conditions
        for condition in EYEWEAR_KEYWORDS["vision_conditions"]:
            if condition.lower() in query_lower:
                intent_info["vision_condition"] = condition
                intent_info["query_type"] = "medical_consultation"
                break
        
        # Detect product interest
        for product in EYEWEAR_KEYWORDS["lens_types"]:
            if product.lower() in query_lower:
                intent_info["product_interest"] = product
                intent_info["query_type"] = "product_recommendation"
                break
        
        # Detect style preference
        for style in EYEWEAR_KEYWORDS["frame_styles"]:
            if style.lower() in query_lower:
                intent_info["style_preference"] = style
                intent_info["query_type"] = "style_consultation"
                break
        
        # Detect technical questions
        technical_terms = ["coating", "l·ªõp ph·ªß", "ch·ªâ s·ªë kh√∫c x·∫°", "ƒë·ªô d√†y", "UV", "blue light"]
        if any(term in query_lower for term in technical_terms):
            intent_info["technical_level"] = "advanced"
        
        return intent_info
    
    def create_domain_prompt(self, query: str, context: str, intent_info: Dict) -> str:
        """
        T·∫°o prompt t·ªëi ∆∞u cho domain m·∫Øt k√≠nh
        """
        base_role = """B·∫°n l√† chuy√™n gia t∆∞ v·∫•n m·∫Øt k√≠nh chuy√™n nghi·ªáp v·ªõi nhi·ªÅu nƒÉm kinh nghi·ªám trong lƒ©nh v·ª±c quang h·ªçc. 
B·∫°n c√≥ ki·∫øn th·ª©c s√¢u r·ªông v·ªÅ:
- C√°c t·∫≠t kh√∫c x·∫° m·∫Øt (c·∫≠n th·ªã, vi·ªÖn th·ªã, lo·∫°n th·ªã, l√£o th·ªã)
- C√°c lo·∫°i tr√≤ng k√≠nh v√† c√¥ng ngh·ªá lens
- Phong c√°ch v√† thi·∫øt k·∫ø g·ªçng k√≠nh
- V·∫≠t li·ªáu v√† c√¥ng ngh·ªá s·∫£n xu·∫•t
- Xu h∆∞·ªõng th·ªùi trang k√≠nh m·∫Øt"""

        if intent_info["query_type"] == "medical_consultation":
            role_specific = """
H√£y t·∫≠p trung v√†o:
- Gi·∫£i th√≠ch r√µ r√†ng t√¨nh tr·∫°ng th·ªã l·ª±c
- ƒê·ªÅ xu·∫•t lo·∫°i tr√≤ng k√≠nh ph√π h·ª£p
- L∆∞u √Ω v·ªÅ vi·ªác thƒÉm kh√°m m·∫Øt ƒë·ªãnh k·ª≥
- Kh√¥ng thay th·∫ø √Ω ki·∫øn b√°c sƒ© chuy√™n khoa"""

        elif intent_info["query_type"] == "product_recommendation":
            role_specific = """
H√£y t·∫≠p trung v√†o:
- So s√°nh ∆∞u nh∆∞·ª£c ƒëi·ªÉm c√°c s·∫£n ph·∫©m
- ƒê·ªÅ xu·∫•t s·∫£n ph·∫©m ph√π h·ª£p v·ªõi nhu c·∫ßu
- Th√¥ng tin v·ªÅ gi√° c·∫£ v√† ch·∫•t l∆∞·ª£ng
- H∆∞·ªõng d·∫´n c√°ch ch·ªçn mua"""

        elif intent_info["query_type"] == "style_consultation":
            role_specific = """
H√£y t·∫≠p trung v√†o:
- Ph√¢n t√≠ch khu√¥n m·∫∑t v√† phong c√°ch ph√π h·ª£p
- Xu h∆∞·ªõng th·ªùi trang m·∫Øt k√≠nh
- C√°ch ph·ªëi h·ª£p v·ªõi trang ph·ª•c
- L·ªùi khuy√™n v·ªÅ m√†u s·∫Øc v√† ch·∫•t li·ªáu"""

        else:
            role_specific = """
H√£y cung c·∫•p th√¥ng tin to√†n di·ªán v√† th·ª±c t·∫ø."""

        context_instruction = f"""
D·ª±a tr√™n th√¥ng tin sau t·ª´ c∆° s·ªü d·ªØ li·ªáu v·ªÅ m·∫Øt k√≠nh:

{context}

C√¢u h·ªèi c·ªßa kh√°ch h√†ng: {query}
"""

        response_guidelines = """
H√£y tr·∫£ l·ªùi theo c·∫•u tr√∫c:
1. **Ph√¢n t√≠ch nhu c·∫ßu**: Hi·ªÉu r√µ t√¨nh hu·ªëng c·ªßa kh√°ch h√†ng
2. **ƒê·ªÅ xu·∫•t c·ª• th·ªÉ**: G·ª£i √Ω s·∫£n ph·∫©m/gi·∫£i ph√°p ph√π h·ª£p  
3. **Gi·∫£i th√≠ch l√Ω do**: T·∫°i sao ƒë√¢y l√† l·ª±a ch·ªçn t·ªët
4. **L∆∞u √Ω quan tr·ªçng**: ƒêi·ªÅu c·∫ßn ch√∫ √Ω khi s·ª≠ d·ª•ng
5. **T∆∞ v·∫•n th√™m**: G·ª£i √Ω v·ªÅ chƒÉm s√≥c ho·∫∑c l·ª±a ch·ªçn kh√°c

L∆∞u √Ω:
- S·ª≠ d·ª•ng ng√¥n ng·ªØ th√¢n thi·ªán, d·ªÖ hi·ªÉu
- ƒê∆∞a ra l·ªùi khuy√™n th·ª±c t·∫ø v√† c√≥ cƒÉn c·ª©
- N·∫øu c·∫ßn thƒÉm kh√°m chuy√™n khoa, h√£y khuy√™n kh√°ch h√†ng ƒëi kh√°m
- Tr√°nh cam k·∫øt ch·ªØa b·ªánh ho·∫∑c k·∫øt qu·∫£ ch·∫Øc ch·∫Øn
"""

        return f"{base_role}\n{role_specific}\n{context_instruction}\n{response_guidelines}"
    
    def enhance_context_with_keywords(self, context: str, intent_info: Dict) -> str:
        """
        TƒÉng c∆∞·ªùng context v·ªõi keywords domain-specific
        """
        enhanced_context = context
        
        # Th√™m keywords li√™n quan d·ª±a tr√™n intent
        if intent_info["vision_condition"]:
            related_keywords = [kw for kw in EYEWEAR_KEYWORDS["vision_conditions"] 
                              if kw != intent_info["vision_condition"]]
            enhanced_context += f"\n\nC√°c t√¨nh tr·∫°ng li√™n quan: {', '.join(related_keywords)}"
        
        if intent_info["product_interest"]:
            related_products = [kw for kw in EYEWEAR_KEYWORDS["lens_types"] 
                              if kw != intent_info["product_interest"]]
            enhanced_context += f"\n\nC√°c lo·∫°i s·∫£n ph·∫©m li√™n quan: {', '.join(related_products[:3])}"
        
        return enhanced_context
    
    def post_process_response(self, response: str, intent_info: Dict) -> str:
        """
        X·ª≠ l√Ω response ƒë·ªÉ ph√π h·ª£p v·ªõi domain
        """
        # Th√™m disclaimer cho medical advice
        if intent_info["query_type"] == "medical_consultation":
            disclaimer = "\n\n‚ö†Ô∏è **L∆∞u √Ω quan tr·ªçng**: Th√¥ng tin n√†y ch·ªâ mang t√≠nh tham kh·∫£o. H√£y thƒÉm kh√°m b√°c sƒ© nh√£n khoa ƒë·ªÉ ƒë∆∞·ª£c ch·∫©n ƒëo√°n v√† t∆∞ v·∫•n ch√≠nh x√°c."
            if disclaimer not in response:
                response += disclaimer
        
        # Th√™m call-to-action cho product recommendations
        if intent_info["query_type"] == "product_recommendation":
            cta = "\n\nüí° **G·ª£i √Ω**: B·∫°n c√≥ th·ªÉ ƒë·∫øn c·ª≠a h√†ng ƒë·ªÉ th·ª≠ tr·ª±c ti·∫øp v√† nh·∫≠n t∆∞ v·∫•n chi ti·∫øt t·ª´ nh√¢n vi√™n chuy√™n nghi·ªáp."
            if "c·ª≠a h√†ng" not in response.lower():
                response += cta
        
        return response
    
    def generate_response(self, query: str, context: str) -> Dict[str, Any]:
        """
        T·∫°o response v·ªõi logic domain-specific cho m·∫Øt k√≠nh
        """
        try:
            # Ph√¢n t√≠ch intent
            intent_info = self.detect_query_intent(query)
            
            # TƒÉng c∆∞·ªùng context
            enhanced_context = self.enhance_context_with_keywords(context, intent_info)
            
            # T·∫°o prompt t·ªëi ∆∞u
            prompt = self.create_domain_prompt(query, enhanced_context, intent_info)
            
            # G·ªçi LLM
            response = self.llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
            
            # Post-process
            final_answer = self.post_process_response(answer, intent_info)
            
            return {
                "answer": final_answer,
                "intent_info": intent_info,
                "status": "success"
            }
            
        except Exception as e:
            return {
                "answer": f"Xin l·ªói, t√¥i g·∫∑p kh√≥ khƒÉn khi x·ª≠ l√Ω c√¢u h·ªèi v·ªÅ m·∫Øt k√≠nh. L·ªói: {str(e)}",
                "intent_info": {"query_type": "error"},
                "status": "error",
                "error": str(e)
            }
    
    def get_health_status(self) -> Dict[str, Any]:
        """
        Ki·ªÉm tra tr·∫°ng th√°i health c·ªßa RAG agent
        """
        try:
            # Test simple query
            test_response = self.llm.invoke("Test connection")
            return {
                "status": "healthy",
                "model": Config.GEMINI_MODEL,
                "domain": Config.DOMAIN,
                "features": {
                    "intent_detection": True,
                    "domain_prompts": True,
                    "medical_disclaimer": True,
                    "product_recommendations": Config.ENABLE_PRODUCT_RECOMMENDATIONS,
                    "technical_advice": Config.ENABLE_TECHNICAL_ADVICE
                }
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }

    def grade_retrieved_documents(self, query: str, documents: List[Dict]) -> List[Dict]:
        """
        B∆∞·ªõc 3.3: ƒê√°nh Gi√° v√† L·ªçc K·∫øt Qu·∫£ Truy Xu·∫•t
        ƒê√°nh gi√° ƒë·ªô li√™n quan c·ªßa documents v·ªõi c√¢u h·ªèi
        """
        print("ƒêang ƒë√°nh gi√° ƒë·ªô li√™n quan c·ªßa documents...")
        
        relevant_docs = []
        
        for doc in documents:
            # Ki·ªÉm tra ƒëi·ªÉm similarity t·ª´ Qdrant
            if doc["score"] >= Config.SIMILARITY_THRESHOLD:
                # Th√™m ƒë√°nh gi√° LLM n·∫øu c·∫ßn (t√πy ch·ªçn)
                grade_prompt = f"""
                H√£y ƒë√°nh gi√° xem ƒëo·∫°n vƒÉn b·∫£n sau c√≥ li√™n quan ƒë·∫øn c√¢u h·ªèi kh√¥ng.
                
                C√¢u h·ªèi: {query}
                
                ƒêo·∫°n vƒÉn b·∫£n: {doc["content"][:500]}...
                
                Tr·∫£ l·ªùi ch·ªâ "YES" n·∫øu li√™n quan ho·∫∑c "NO" n·∫øu kh√¥ng li√™n quan.
                """
                
                try:
                    messages = [HumanMessage(content=grade_prompt)]
                    response = self.llm.invoke(messages)
                    grade = response.content.strip().upper()
                    
                    if grade == "YES":
                        relevant_docs.append(doc)
                        print(f"‚úì Document t·ª´ {doc['source']} - chunk {doc['chunk_id']} ƒë∆∞·ª£c ch·∫•p nh·∫≠n (score: {doc['score']:.3f})")
                    else:
                        print(f"‚úó Document t·ª´ {doc['source']} - chunk {doc['chunk_id']} b·ªã lo·∫°i b·ªè (kh√¥ng li√™n quan)")
                        
                except Exception as e:
                    # N·∫øu l·ªói LLM, ch·ªâ d·ª±a v√†o ƒëi·ªÉm similarity
                    relevant_docs.append(doc)
                    print(f"‚ö† L·ªói ƒë√°nh gi√° Gemini, gi·ªØ document d·ª±a tr√™n similarity score: {e}")
            else:
                print(f"‚úó Document t·ª´ {doc['source']} - chunk {doc['chunk_id']} b·ªã lo·∫°i b·ªè (score th·∫•p: {doc['score']:.3f})")
        
        print(f"ƒê√£ l·ªçc ƒë∆∞·ª£c {len(relevant_docs)}/{len(documents)} documents li√™n quan")
        return relevant_docs
    
    def aggregate_context(self, documents: List[Dict]) -> str:
        """
        B∆∞·ªõc 3.4: T·ªïng H·ª£p Ng·ªØ C·∫£nh (Context Aggregation)
        K·∫øt h·ª£p n·ªôi dung c√°c documents th√†nh context th·ªëng nh·∫•t
        """
        if not documents:
            return ""
        
        print("ƒêang t·ªïng h·ª£p ng·ªØ c·∫£nh t·ª´ documents...")
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            source_info = f"[Ngu·ªìn: {doc['source']}, Ph·∫ßn {doc['chunk_id']}]"
            content = f"{source_info}\n{doc['content']}\n"
            context_parts.append(content)
        
        context = "\n" + "="*50 + "\n".join(context_parts)
        
        print(f"ƒê√£ t·ªïng h·ª£p context t·ª´ {len(documents)} documents")
        return context
    
    def generate_answer(self, query: str, context: str) -> str:
        """
        B∆∞·ªõc 3.5: T·∫°o C√¢u Tr·∫£ L·ªùi v·ªõi Gemini LLM (Answer Generation with LLM)
        S·ª≠ d·ª•ng Google Gemini ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi d·ª±a tr√™n context
        """
        if not context.strip():
            return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫°n."
        
        print("ƒêang t·∫°o c√¢u tr·∫£ l·ªùi v·ªõi Google Gemini...")
        
        # Gemini works better with a single comprehensive prompt
        full_prompt = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, chuy√™n tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.

H∆∞·ªõng d·∫´n quan tr·ªçng:
1. Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p
2. Kh√¥ng t·ª± suy di·ªÖn ho·∫∑c th√™m th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu
3. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥
4. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng v√† d·ªÖ hi·ªÉu
5. Tr√≠ch d·∫´n ngu·ªìn khi c√≥ th·ªÉ (t√™n file, ph·∫ßn)
6. N·∫øu c√≥ nhi·ªÅu ngu·ªìn th√¥ng tin, h√£y t·ªïng h·ª£p m·ªôt c√°ch logic

Ng·ªØ c·∫£nh t·ª´ t√†i li·ªáu:
{context}

C√¢u h·ªèi: {query}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh tr√™n. ƒê·ªãnh d·∫°ng tr·∫£ l·ªùi:
- C√¢u tr·∫£ l·ªùi ch√≠nh
- Ngu·ªìn tham kh·∫£o (n·∫øu c√≥)
"""
        
        try:
            messages = [HumanMessage(content=full_prompt)]
            response = self.llm.invoke(messages)
            answer = response.content.strip()
            
            print("‚úì ƒê√£ t·∫°o c√¢u tr·∫£ l·ªùi v·ªõi Gemini th√†nh c√¥ng")
            return answer
            
        except Exception as e:
            error_msg = f"L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi v·ªõi Gemini: {str(e)}"
            print(f"‚úó {error_msg}")
            return f"Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n: {error_msg}"
    
    def process_query(self, query: str, retrieved_documents: List[Dict]) -> Dict:
        """
        X·ª≠ l√Ω ho√†n ch·ªânh m·ªôt query: ƒë√°nh gi√° docs -> t·ªïng h·ª£p context -> t·∫°o answer
        """
        # B∆∞·ªõc 3.3: ƒê√°nh gi√° v√† l·ªçc documents
        relevant_docs = self.grade_retrieved_documents(query, retrieved_documents)
        
        # B∆∞·ªõc 3.4: T·ªïng h·ª£p context
        context = self.aggregate_context(relevant_docs)
        
        # B∆∞·ªõc 3.5: T·∫°o c√¢u tr·∫£ l·ªùi
        answer = self.generate_answer(query, context)
        
        return {
            "query": query,
            "answer": answer,
            "relevant_documents_count": len(relevant_docs),
            "total_retrieved_count": len(retrieved_documents),
            "sources": list(set([doc["source"] for doc in relevant_docs])),
            "context": context
        } 